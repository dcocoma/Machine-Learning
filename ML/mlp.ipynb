{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl76ZKfkRCS8"
      },
      "source": [
        "**Group : 1**\n",
        "\n",
        "**COCOMA REYES, David Leonardo**\n",
        "\n",
        "**DEJEAN, Maxime**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN2U5dB5RCS-"
      },
      "source": [
        "## Advice\n",
        "### Work\n",
        "- **Read the whole subject before starting**\n",
        "- Search for the documentation (Google) before asking a teacher\n",
        "- The work will be evaluated based on this notebook:\n",
        "    - Answer questions in the notebook.\n",
        "    - Insert your code here and execute it so that the output stays displayed for the teacher.\n",
        "\n",
        "### ChatGPT\n",
        "- You can use ChatGPT ONLY to answer specific questions, get introductory explanations on machine learning libraries, get example codes. Be aware that there is no guaranty in the answer of ChatGPT (even with the paying licence).\n",
        "- Do not use ChatGPT to work in your stead ! The goal is for you to learn the manipulation of machine learning basic methods yourself. Teachers are used to look at student's works and ChatGPT is not good to fake it.\n",
        "=> In case of a doubt of ChatGPT use (or plagiarism between groups), the students will be summoned and evaluated on an oral presentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhFbVBS8RCS_"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecyw1Q13RCS_"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The objective of this lab is to dive into particular kind of neural network: the *Multi-Layer Perceptron* (MLP) (slide 53 of the slides)\n",
        "\n",
        "To start, let us take the dataset from the previous lab (hydrodynamics of sailing boats) and use scikit-learn to train a MLP instead of our hand-made single perceptron.\n",
        "The code below is already complete and is meant to give you an idea of how to construct an MLP with scikit-learn. You can execute it, taking the time to understand the idea behind each cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "yuTbLa82RCS_",
        "outputId": "792c33e2-40fd-4abe-fb84-a03e01694ce0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "yacht_hydrodynamics.data not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0ff2f0132f84>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Importing the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yacht_hydrodynamics.data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1980\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1981\u001b[0m         \u001b[0mfid_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: yacht_hydrodynamics.data not found."
          ]
        }
      ],
      "source": [
        "# Importing the dataset\n",
        "import numpy as np\n",
        "dataset = np.genfromtxt(\"yacht_hydrodynamics.data\", delimiter='')\n",
        "X = dataset[:, :-1]\n",
        "Y = dataset[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M702y01XfS_S",
        "outputId": "b60dc41a-71f6-4237-d974-87517a4a32b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "aJDAItKHRCTD",
        "outputId": "21f483c7-1d45-4b5b-c700-94acddfa2a6f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-92607efbf738>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "# Preprocessing: scale input data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWadmrRYRCTD"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y,random_state=1, test_size = 0.20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMBSHVbERCTD"
      },
      "outputs": [],
      "source": [
        "# Define a multi-layer perceptron (MLP) network for regression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "mlp = MLPRegressor(max_iter=3000, random_state=1) # define the model, with default params\n",
        "mlp.fit(x_train, y_train) # train the MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRmMPCqeRCTE"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "print('Train score: ', mlp.score(x_train, y_train))\n",
        "print('Test score:  ', mlp.score(x_test, y_test))\n",
        "plt.plot(mlp.loss_curve_)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BM0NB6ZRCTE"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "num_samples_to_plot = 50\n",
        "plt.plot(y_test[0:num_samples_to_plot], 'ro', label='y')\n",
        "yw = mlp.predict(x_test)\n",
        "plt.plot(yw[0:num_samples_to_plot], 'bx', label='$\\hat{y}$')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Examples\")\n",
        "plt.ylabel(\"f(examples)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAVQ7TI8RCTF"
      },
      "source": [
        "### Analyzing the network\n",
        "\n",
        "Many details of the network are currently hidden as default parameters.\n",
        "\n",
        "Using the [documentation of the MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html), answer the following questions.\n",
        "\n",
        "- What is the structure of the network?\n",
        "\n",
        "Its a Multi-layer Perceptron regressor. This model optimizes the squared error using LBFGS or stochastic gradient descent.\n",
        "- What it is the algorithm used for training? Is there algorithm available that we mentioned during the courses?\n",
        "\n",
        "The algorithm uses by default the solver 'adam' and it refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba but for in the courses we qre using a clasic stochastic gradient-based optimize wich can be used with the solver ‘sgd’ that refers to stochastic gradient descent.\n",
        "- How does the training algorithm decides to stop the training?\n",
        "\n",
        "We can define the max iterations in the begining to limit the learning if its not converging, but the solver iterates until the loss value its smaller than ‘tol’ wich is a tolfloat that we can configure but seted by default at 1e-4 or when the loss or score is not improving by at least tol for n_iter_no_change consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is considered to be reached and training stops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOPo7rYvRCTF"
      },
      "source": [
        "# Onto a more challenging dataset: house prices\n",
        "\n",
        "For the rest of this lab, we will use the (more challenging) [California Housing Prices dataset](https://www.kaggle.com/datasets/camnugent/california-housing-prices)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lowuRBzRCTF"
      },
      "outputs": [],
      "source": [
        "# clean all previously defined variables for the sailing boats\n",
        "%reset -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wnGdNGeRCTG"
      },
      "outputs": [],
      "source": [
        "\"\"\"Import the required modules\"\"\"\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "num_samples = 2000\n",
        "\n",
        "cal_housing = fetch_california_housing()\n",
        "print(type(cal_housing))\n",
        "X_all = pd.DataFrame(cal_housing.data,columns=cal_housing.feature_names)\n",
        "y_all = cal_housing.target\n",
        "\n",
        "X_all, y_all = shuffle(X_all, y_all, random_state=42)\n",
        "\n",
        "# only use the first N samples to limit training time\n",
        "X, Y = X_all[:num_samples], y_all[:num_samples]\n",
        "\n",
        "X.head(10) # print the first 10 values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrJ_n1Z9RCTG"
      },
      "source": [
        "Note that each row of the dataset represents a **group of houses** (one district). The `target` variable denotes the average house value in units of 100.000 USD. Median Income is per 10.000 USD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H6uy6oIRCTG"
      },
      "source": [
        "### Extracting a subpart of the dataset for testing\n",
        "\n",
        "- Split the dataset between a training set (75%) and a test set (25%)\n",
        "\n",
        "Please use the conventional names `X_train`, `X_test`, `y_train` and `y_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msaJK-NpRCTG"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y,random_state=1, test_size = 0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mqg62i6QRCTG"
      },
      "source": [
        "### Scaling the input data\n",
        "\n",
        "\n",
        "A step of **scaling** of the data is often useful to ensure that all input data centered on 0 and with a fixed variance.\n",
        "\n",
        "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). The function `StandardScaler` from `sklearn.preprocessing` computes the standard score of a sample as:\n",
        "\n",
        "```\n",
        "z = (x - u) / s\n",
        "```\n",
        "\n",
        "where `u` is the mean of the training samples, and `s` is the standard deviation of the training samples.\n",
        "\n",
        "Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using transform.\n",
        "\n",
        " - Apply the standard scaler to both the training dataset (`X_train`) and the test dataset (`X_test`).\n",
        " - Make sure that **exactly the same transformation** is applied to both datasets.\n",
        "\n",
        "[Documentation of standard scaler in scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYaHZ1DARCTH"
      },
      "outputs": [],
      "source": [
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train) #fit calcule le u et le s; transform l'applique\n",
        "x_test = sc.transform(x_test)  #pas besoin de refit car sinon on aura pas les memes transform de test et train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq_3LyQlRCTH"
      },
      "source": [
        "## Overfitting\n",
        "\n",
        "In this part, we are only interested in maximizing the **train score**, i.e., having the network memorize the training examples as well as possible.\n",
        "\n",
        "- Propose a parameterization of the network (shape and learning parameters) that will maximize the train score (without considering the test score).\n",
        "\n",
        "While doing this, you should (1) remain within two minutes of training time, and (2) obtain a score that is greater than 0.90.\n",
        "\n",
        "- Is the **test** score substantially smaller than the **train** score (indicator of overfitting) ?\n",
        "- Explain how the parameters you chose allow the learned model to overfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFJT_ReCRCTH"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(200,100),max_iter=3000, solver='adam',tol=0.0001, random_state=1) # define the model\n",
        "mlp.fit(x_train, y_train) # train the MLP\n",
        "\n",
        "print('Train score: ', mlp.score(x_train, y_train))\n",
        "print('Test score:  ', mlp.score(x_test, y_test))\n",
        "plt.plot(mlp.loss_curve_)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "#on a de l'overfitting test score << train score car complexité du réseau (2 hidden layers de grande dimensions) trop importantes\n",
        "#par rapport au nombre de paramètres et la taille du dataset utilisé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM8qfgEGRCTH"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "In this section, we are now interested in maximizing the ability of the network to predict the value of unseen examples, i.e., maximizing the **test** score.\n",
        "You should experiment with the possible parameters of the network in order to obtain a good test score, ideally with a small learning time.\n",
        "**Explore 10 different configurations**.\n",
        "\n",
        "Parameters to vary:\n",
        "\n",
        "- number and size of the hidden layers\n",
        "- activation function\n",
        "- stopping conditions\n",
        "- maximum number of iterations\n",
        "- initial learning rate value\n",
        "\n",
        "Results to present for the tested configurations:\n",
        "\n",
        "- Train/test score\n",
        "- training time\n",
        "\n",
        "Two constraints:\n",
        "1) Present in a table the various parameters tested and the associated results. You can find in the last cell of the notebook a code snippet that will allow you to plot tables from python structure.\n",
        "2) Set up an automatization in the way your run your experiments and collect data. For each run, the output must stay displayed in the notebook, and you should record the parameters and results into an external data structure. **The structure *data* can typically be used for automatization and storing !**\n",
        "\n",
        "(Note that, while we encourage you to explore the solution space manually, there are existing methods in scikit-learn and other learning framework to automate this step as well, e.g., [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQyBAleGRCTI"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(200,100),max_iter=3000, solver='adam',tol=0.0001, random_state=1) # define the model\n",
        "mlp.fit(x_train, y_train) # train the MLP\n",
        "print('Train score: ', mlp.score(x_train, y_train))\n",
        "print('Test score:  ', mlp.score(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIFTqWjIRCTI"
      },
      "outputs": [],
      "source": [
        "# Code snippet to display a nice table in jupyter notebooks  (remove from report)\n",
        "import pandas as pd\n",
        "data = []\n",
        "data.append({'hidden_layer_sizes': (200,100), 'activation': 'relu', 'learning_rate_init': 0.001, 'max_iter': 500})\n",
        "data.append({'hidden_layer_sizes': (100,50), 'activation': 'relu', 'learning_rate_init': 0.001, 'max_iter': 500})\n",
        "data.append({'hidden_layer_sizes': (50,25), 'activation': 'relu', 'learning_rate_init': 0.001, 'max_iter': 500})\n",
        "data.append({'hidden_layer_sizes': (200,100),'activation': 'tanh', 'learning_rate_init': 0.001, 'max_iter': 500})\n",
        "data.append({'hidden_layer_sizes': (100,50), 'activation': 'tanh', 'learning_rate_init': 0.001, 'max_iter': 500})\n",
        "data.append({'hidden_layer_sizes': (50,25), 'activation': 'tanh', 'learning_rate_init': 0.001, 'max_iter': 500})\n",
        "data.append({'hidden_layer_sizes': (200,100),'activation': 'relu', 'learning_rate_init': 0.01, 'max_iter': 500})\n",
        "data.append({'hidden_layer_sizes': (100,50), 'activation': 'tanh', 'learning_rate_init': 0.002, 'max_iter': 300})\n",
        "data.append({'hidden_layer_sizes': (50,25), 'activation': 'tanh', 'learning_rate_init': 0.02, 'max_iter': 500})\n",
        "data.append({'hidden_layer_sizes': (50,25), 'activation': 'relu', 'learning_rate_init': 0.02, 'max_iter': 500})\n",
        "\n",
        "for d in data :\n",
        "    mlp = MLPRegressor(hidden_layer_sizes=d['hidden_layer_sizes'],\n",
        "                       activation = d['activation'],\n",
        "                       max_iter=d['max_iter'],\n",
        "                       learning_rate_init = d['learning_rate_init'],\n",
        "                       random_state=1\n",
        "                      )\n",
        "    mlp.fit(x_train, y_train) # train the MLP\n",
        "    d['test_score'] = mlp.score(x_test, y_test)\n",
        "    d['train_score'] = mlp.score(x_train, y_train)\n",
        "table = pd.DataFrame.from_dict(data)\n",
        "table = table.replace(np.nan, '-')\n",
        "table = table.sort_values(by='test_score', ascending=False)\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKlF_g3qRCTI"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "- From your experiments, what seems to be the best model (i.e. set of parameters) for predicting the value of a house?\n",
        "\n",
        "Unless you used cross-validation, you have probably used the \"test\" set to select the best model among the ones you experimented with.\n",
        "Since your model is the one that worked best on the \"test\" set, your selection is *biased*.\n",
        "\n",
        "In all rigor the original dataset should be split in three:\n",
        "\n",
        "- the **training set**, on which each model is trained\n",
        "- the **validation set**, that is used to pick the best parameters of the model\n",
        "- the **test set**, on which we evaluate the final model\n",
        "\n",
        "\n",
        "Evaluate the score of your algorithm on a test set that was not used for training nor for model selection. Note that only 2000 samples were used from the full california_housing dataset, there are a lot of remaining (see data loading at the beginning)..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4nP80s_RCTI"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TR-ws_2RCTI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}